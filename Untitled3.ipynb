{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "33dc6d49505b4536b6a128d9d7c879e1fa44477ad44947bbbe73093067fe6393"
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvcFLx0mWNL8"
      },
      "source": [
        "utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK-BdROaWA9H"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    xt = np.exp(x - np.max(x))\n",
        "    return xt / np.sum(xt)\n",
        "\n",
        "def save_model_parameters_theano(outfile, model):\n",
        "    U, V, W = model.U.get_value(), model.V.get_value(), model.W.get_value()\n",
        "    np.savez(outfile, U=U, V=V, W=W)\n",
        "    print (outfile)\n",
        "    print (\"Saved model parameters to %s.\" % outfile)\n",
        "   \n",
        "def load_model_parameters_theano(path, model):\n",
        "    npzfile = np.load(path)\n",
        "    U, V, W = npzfile[\"U\"], npzfile[\"V\"], npzfile[\"W\"]\n",
        "    model.hidden_dim = U.shape[0]\n",
        "    model.word_dim = U.shape[1]\n",
        "    model.U.set_value(U)\n",
        "    model.V.set_value(V)\n",
        "    model.W.set_value(W)\n",
        "    print(\"Loaded model parameters from %s. hidden_dim=%d word_dim=%d\" % (path, U.shape[0], U.shape[1]))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQY9a3OeWP3J"
      },
      "source": [
        "mn_theano"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4mkwA-xWR4i"
      },
      "source": [
        "import numpy as np\n",
        "import theano as theano\n",
        "import theano.tensor as T\n",
        "class RNNTheano:\n",
        "    \n",
        "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
        "        # Assign instance variables\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        # Randomly initialize the network parameters\n",
        "        U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "        # Theano: Created shared variables\n",
        "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
        "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
        "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))      \n",
        "        # We store the Theano graph here\n",
        "        self.theano = {}\n",
        "        self.__theano_build__()\n",
        "    \n",
        "    def __theano_build__(self):\n",
        "        U, V, W = self.U, self.V, self.W\n",
        "        x = T.ivector('x')\n",
        "        y = T.ivector('y')\n",
        "        def forward_prop_step(x_t, s_t_prev, U, V, W):\n",
        "            s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))\n",
        "            o_t = T.nnet.softmax(V.dot(s_t))\n",
        "            return [o_t[0], s_t]\n",
        "        [o,s], updates = theano.scan(\n",
        "            forward_prop_step,\n",
        "            sequences=x,\n",
        "            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n",
        "            non_sequences=[U, V, W],\n",
        "            truncate_gradient=self.bptt_truncate,\n",
        "            strict=True)\n",
        "        \n",
        "        prediction = T.argmax(o, axis=1)\n",
        "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
        "        \n",
        "        # Gradients\n",
        "        dU = T.grad(o_error, U)\n",
        "        dV = T.grad(o_error, V)\n",
        "        dW = T.grad(o_error, W)\n",
        "        \n",
        "        # Assign functions\n",
        "        self.forward_propagation = theano.function([x], o)\n",
        "        self.predict = theano.function([x], prediction)\n",
        "        self.ce_error = theano.function([x, y], o_error)\n",
        "        self.bptt = theano.function([x, y], [dU, dV, dW])\n",
        "        \n",
        "        # SGD\n",
        "        learning_rate = T.scalar('learning_rate')\n",
        "        self.sgd_step = theano.function([x,y,learning_rate], [], \n",
        "                      updates=[(self.U, self.U - learning_rate * dU),\n",
        "                              (self.V, self.V - learning_rate * dV),\n",
        "                              (self.W, self.W - learning_rate * dW)])\n",
        "    \n",
        "    def calculate_total_loss(self, X, Y):\n",
        "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
        "    \n",
        "    def calculate_loss(self, X, Y):\n",
        "        # Divide calculate_loss by the number of words\n",
        "        num_words = np.sum([len(y) for y in Y])\n",
        "        return self.calculate_total_loss(X,Y)/float(num_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
            "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
            "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
            "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
            "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEHh5PV8WZCh"
      },
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aP4kkfjWbjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c248b3b8-9e79-40a2-e434-92841f1a13ed"
      },
      "source": [
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "nltk.download(\"book\")\n",
        "\n",
        "_VOCABULARY_SIZE = int(os.environ.get('VOCABULARY_SIZE', '8000'))\n",
        "_HIDDEN_DIM = int(os.environ.get('HIDDEN_DIM', '80'))\n",
        "_LEARNING_RATE = float(os.environ.get('LEARNING_RATE', '0.005'))\n",
        "_NEPOCH = int(os.environ.get('NEPOCH', '100'))\n",
        "_MODEL_FILE = os.environ.get('MODEL_FILE')\n",
        "\n",
        "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=1, evaluate_loss_after=5):\n",
        "    # We keep track of the losses so we can plot them later\n",
        "    losses = []\n",
        "    num_examples_seen = 0\n",
        "    for epoch in range(nepoch):\n",
        "        print(epoch)\n",
        "        # Optionally evaluate the loss\n",
        "        if (epoch % evaluate_loss_after == 0):\n",
        "            loss = model.calculate_loss(X_train, y_train)\n",
        "            losses.append((num_examples_seen, loss))\n",
        "            time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
        "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
        "            # Adjust the learning rate if loss increases\n",
        "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
        "                learning_rate = learning_rate * 0.5  \n",
        "                print(\"Setting learning rate to %f\" % learning_rate)\n",
        "            sys.stdout.flush()\n",
        "            # ADDED! Saving model oarameters\n",
        "            save_model_parameters_theano(\"/rnn-theano-%d-%d-%s.npz\" % (model.hidden_dim, model.word_dim, time), model)\n",
        "        # For each training example...\n",
        "        for i in range(len(y_train)):\n",
        "            # One SGD step\n",
        "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
        "            num_examples_seen += 1\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UOv43zOWiNl"
      },
      "source": [
        "vocabulary_size = _VOCABULARY_SIZE\n",
        "unknown_token = \"UNKNOWN_TOKEN\"\n",
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jI2q5QHXOY8",
        "outputId": "7207e54d-6fd6-44d1-8221-c0a37709668b"
      },
      "source": [
        "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
        "print(\"Reading CSV file...\")\n",
        "with open('reddit-comments-2015-08.csv', 'rt', encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f, skipinitialspace=True)\n",
        "    next(reader)\n",
        "    # Split full comments into sentences\n",
        "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
        "    # Append SENTENCE_START and SENTENCE_END\n",
        "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "print(\"Parsed %d sentences.\" % (len(sentences)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading CSV file...\n",
            "Parsed 79170 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQjEs6YzWmCY",
        "outputId": "80758961-8e7b-4ec0-ef42-8b331c8f9af3"
      },
      "source": [
        "# Tokenize the sentences into words\n",
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Count the word frequencies\n",
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
        "\n",
        "# Get the most common words and build index_to_word and word_to_index vectors\n",
        "vocab = word_freq.most_common(vocabulary_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
        "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 63022 unique words tokens.\nUsing vocabulary size 8000.\nThe least frequent word in our vocabulary is 'appointments' and appeared 10 times.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWGaFRD4WqoP"
      },
      "source": [
        "# Replace all words not in our vocabulary with the unknown token\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIyuigNDWrhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50f9d23-a371-4885-f578-30dbb92b3a55"
      },
      "source": [
        "# Create the training data\n",
        "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
        "\n",
        "model = RNNTheano(vocabulary_size, hidden_dim=_HIDDEN_DIM)\n",
        "t1 = time.time()\n",
        "model.sgd_step(X_train[10], y_train[10], _LEARNING_RATE)\n",
        "t2 = time.time()\n",
        "print (\"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.))\n",
        "\n",
        "\n",
        "model = RNNTheano(vocabulary_size, hidden_dim=50)\n",
        "\n",
        "load_model_parameters_theano('trained-model-theano.npz', model)\n",
        "\n",
        "#Tạo câu ngẫu nhiên\n",
        "def generate_sentence(model):\n",
        "    new_sentence = [word_to_index[sentence_start_token]]\n",
        "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
        "        next_word_probs = model.forward_propagation(new_sentence)\n",
        "        sampled_word = word_to_index[unknown_token]\n",
        "        while sampled_word == word_to_index[unknown_token]:\n",
        "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
        "            sampled_word = np.argmax(samples)\n",
        "        new_sentence.append(sampled_word)\n",
        "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
        "    return sentence_str\n",
        "\n",
        "#Đoán các từ tiếp theo từ câu cho trước\n",
        "def guess_sentence(sentence, model):\n",
        "  sentence = [\"%s %s\" % (sentence_start_token, sentence)]\n",
        "  tokenized_sentence = nltk.word_tokenize(sentence[0])\n",
        "  tokenized_sentence = [w if w in word_to_index else unknown_token for w in tokenized_sentence]\n",
        "  guess_sent = [word_to_index[w] for w in tokenized_sentence]\n",
        "  while not guess_sent[-1] == word_to_index[sentence_end_token]:\n",
        "    next_word_probs = model.forward_propagation(guess_sent)\n",
        "    sampled_word = word_to_index[unknown_token]\n",
        "    while sampled_word == word_to_index[unknown_token]:\n",
        "      samples = np.random.multinomial(1, next_word_probs[-1])\n",
        "      sampled_word = np.argmax(samples)\n",
        "    guess_sent.append(sampled_word)\n",
        "  sentence_str = [index_to_word[x] for x in guess_sent[1:-1]]\n",
        "  return sentence_str\n",
        "\n",
        "\n",
        "#train_with_sgd(model, X_train, y_train, nepoch=_NEPOCH, learning_rate=_LEARNING_RATE)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<ipython-input-2-1fa5acf58a01>:29: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n",
            "  o_t = T.nnet.softmax(V.dot(s_t))\n",
            "SGD Step time: 13767.599583 milliseconds\n",
            "<ipython-input-2-1fa5acf58a01>:29: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n",
            "  o_t = T.nnet.softmax(V.dot(s_t))\n",
            "Loaded model parameters from trained-model-theano.npz. hidden_dim=50 word_dim=8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFCOntEWt1aQ"
      },
      "source": [
        "run this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dpdiOxRt0g0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b422e29-1b85-4e40-8b74-35e4e67a9eed"
      },
      "source": [
        "num_sentences = 10\n",
        "senten_min_length = 5\n",
        "\n",
        "for i in range(num_sentences):\n",
        "    sent = []\n",
        "    while len(sent) < senten_min_length:\n",
        "        sent = guess_sentence(\"i study \", model)\n",
        "    print (\" \".join(sent))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i study that `` be `` .\n",
            "i study of 7 .\n",
            "i study cup of path they a favorite .\n",
            "i study how fuck all it .\n",
            "i study of drawing rpm downvote .\n",
            "i study of the saying moderators .\n",
            "i study of the rarely in with .\n",
            "i study of nutrition .\n",
            "i study my exist .\n",
            "i study be outside .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from flask import Flask,render_template\n",
        "app = Flask(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @app.route('/')\n",
        "import nltk\n",
        "def change(input_text):\n",
        "    \n",
        "    return 1\n",
        "    # num_sentences = 10\n",
        "    # senten_min_length = 5\n",
        "\n",
        "    # for i in range(num_sentences):\n",
        "    #     sent = []\n",
        "    #     while len(sent) < senten_min_length:\n",
        "    #         sent = guess_sentence(input_text, model)\n",
        "    #     return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "   WARNING: This is a development server. Do not use it in a production deployment.\n",
            "   Use a production WSGI server instead.\n",
            " * Debug mode: off\n",
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
            "127.0.0.1 - - [01/Apr/2021 15:51:58] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [01/Apr/2021 15:52:00] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [01/Apr/2021 15:52:02] \"\u001b[33mPOST /~/Untitled3.ipynb HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "@app.route('/')\n",
        "def main():\n",
        "    return render_template('index.html')\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}